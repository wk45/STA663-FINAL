{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Report I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team Member: Woo Min Kim, Shen Yan <br\\> Papar Selected: Stochastic Gradient Hamiltonian Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief description of the algorithm: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Hamiltonian Monte Carlo sampling algorithm, one can define a Hamiltonian given the target distribution. By letting the state points evolve according to the Hamiltonian dynamics, the state space can be efficiently explored. However, Hamiltonian Monte Carlo requires the computation of the gradient of the target distribution, and when facing with problems with large data set, such computation can become infeasible. Stichastic Gradient Hamiltonian Monte Carlo addresses this problem by using only a minibatch of the data set to compute the gradient. This approximation scheme will bring extra noise to the result, and unfortunately a naive implementation of the stochastic approximation will render the algorithm unable to converge the desired distribution. By introducing a friction term in evolution equations, which turns the dynamics into the second-order Langevin dynamics, this term would magically offset the noise caused by the stochastic approximation, and the algorithm would converge to the target distribution.  <br\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first wrote a version of SGHMC which can only sample one-dimensional random variables. As a simple illustration, We tested it by sampling from a distribution described in the paper, with $U(\\theta) = -2\\theta^2 + \\theta^4$. Then we extend this version to include multi-dimensional random variables, and tested it by sampling from a multivariate normal distribution. So far, we have only optimized the code by trying to write everything in a more pythonic style. Before any further optimization, we tested our sampling algorithm by doing Bayesian logistic regression on a simulated data set, as well as WPBC data set from UCI Machine Learning Repository. For all the tests above, we have written a HMC algorithm to compare. <br\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tried optimizing the algorithm by writing it in c++ without any luck: both HMC and SGHMC need the potential function derived from the target distribution as an input, and we could not find a way to make a function defined in python callable to a function wrapped in c++. We will try to use numba and multiprocessing to further optimize the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
